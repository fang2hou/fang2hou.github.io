<!doctype html><html dir=ltr lang=zh-cn data-theme><head><title>fang2hou | 使用 Google Colab 提供的免费 TPU 进行训练</title><meta charset=utf-8><meta name=generator content="Hugo 0.81.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="硕士研究生 @ 筑波大学
全栈工程师
"><link rel=stylesheet href=/css/style.min.51d7ec6db610dce512b735d5315f963c278042d026aa4ebe05a9355460b49016.css integrity="sha256-UdfsbbYQ3OUStzXVMV+WPCeAQtAmqk6+Bak1VGC0kBY=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.f798cbda9aaa38f89eb38be6414bd082cfd71a6780375cbf67b6d2fb2b96491e.css integrity="sha256-95jL2pqqOPies4vmQUvQgs/XGmeAN1y/Z7bS+yuWSR4=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/fang2hou.min.3a6dc10aeda5ce6aaf464a171a4964b56c7fd3e378469edbb9778362a8a8c2fe.css integrity="sha256-Om3BCu2lzmqvRkoXGklktWx/0+N4Rp7buXeDYqiowv4=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=/post/use-tpu-to-train-cnn/><script type=text/javascript src=/js/anatole-header.min.0c05c0a90d28c968a1cad4fb31abd0b8e1264e788ccefed022ae1d3b6f627514.js integrity="sha256-DAXAqQ0oyWihytT7MavQuOEmTniMzv7QIq4dO29idRQ=" crossorigin=anonymous></script><script type=text/javascript src=/js/anatole-theme-switcher.min.7fd87181cdd7e8413aa64b6867bb32f3a8dc242e684fc7d5bbb9f600dbc2b6eb.js integrity="sha256-f9hxgc3X6EE6pktoZ7sy86jcJC5oT8fVu7n2ANvCtus=" crossorigin=anonymous></script><script type=text/javascript src=/js/custom.min.acdf45cf9e78cef889b7b23c128ac36a7dd95e6463804d224b7edb5f9f4d3535.js integrity="sha256-rN9Fz554zviJt7I8EorDan3ZXmRjgE0iS37bX59NNTU=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="使用 Google Colab 提供的免费 TPU 进行训练"><meta name=twitter:description content="前言
比起深度学习最初的 CPU 计算，现在普遍使用 GPU 来进行训练，效率提升已然是相当明显。但是像是 Google 和阿里巴巴这些科技巨头还是不满足这些，直接让日常使用介入芯片研发，Google 提出 TPU 方案，阿里也成立了平头哥独立品牌来发布新品。官方说法是对于目前顶尖 GPU 都有 20 倍以上的算力提升。
阿里和 Google 的芯片技术都已经实装运用在了自家的云服务上，不过有幸的是，Google 在自家推出的 Colab 机器学习平台之中提供了免费试用 TPU 的机会。这篇文章将会介绍一下当下，快速使用 Keras 进行训练的一个实例。
在看下面的文字的之前，最好回忆一下简单的 CNN 构造。如果你有一些 Keras 或是 TensorFlow 相关的使用经验，那么应该能够非常快速的理解。"></head><body><div class="sidebar animated fadeInDown"><div class=logo-title><div class=title><div class=avatar><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 61.47 85.63"><defs><style>.cls-1,.cls-2{fill:none}.cls-1,.cls-2,.cls-3,.cls-4{stroke:#231f20;stroke-miterlimit:10}.cls-2,.cls-4{stroke-linecap:round}</style></defs><g id="Layer_2" data-name="Layer 2"><g id="Layer_1-2" data-name="Layer 1"><path class="cls-1" d="M31.91 85.13c6.64-.19 11.15-5 14.08-8.15a34.76 34.76.0 008-15.11c1.09-4 .87-8.1 1.51-12.18C56.08 46 56.84 45.12 58 41l.13-.45a111.64 111.64.0 002.81-13.95c.46-6.7-4.42-14.78-10.36-19.65C41-1 29.61.49 27 .83A32.75 32.75.0 008.66 9.72c-2.31 2.26-6.13 6-7.56 12-2 8.23 1.37 17.47 3.26 21 0 .09.4.79.79 1.78.0.0.32.79.59 1.61A37.5 37.5.0 017.13 54.5 38.17 38.17.0 009.84 65.87a31.37 31.37.0 008.3 12.74c2.86 2.56 7.46 6.7 13.77 6.52z"/><path class="cls-2" d="M52.5 36.25c-5.33-5.34-14.37-1.81-14.37-1.81"/><path class="cls-2" d="M10.33 36.25c5.33-5.34 14.37-1.81 14.37-1.81"/><path class="cls-1" d="M15.74 54.45c.28.07 6.37 1.43 10.24-2.84.18-.19 3.84-4.33 2.25-7.91-1.45-3.28-6.19-3.59-8.86-3.77-3.5-.23-9.22.44-10.18 3.27a8.9 8.9.0 00.16 2.27c0 .1.0.21.0.32a33.7 33.7.0 001.13 5.31C11.85 53.51 14.8 54.22 15.74 54.45z"/><path class="cls-1" d="M47.28 54.45c-.28.07-6.36 1.43-10.23-2.84-.18-.19-3.84-4.33-2.25-7.91 1.45-3.28 6.18-3.59 8.86-3.77 3.5-.23 9.22.44 10.18 3.27.24.73-.93 7.16-1.34 7.9C51.18 53.51 48.23 54.22 47.28 54.45z"/><line class="cls-1" x1="28.63" y1="45.79" x2="34.4" y2="45.79"/><path class="cls-2" d="M23.51 69a24.85 24.85.0 007.92 1.55 23.63 23.63.0 008.09-1.39"/><path class="cls-1" d="M58 41a11.27 11.27.0 011.32 3.14c.55 2.19.23 4-.44 7.41-.75 3.86-1.47 4.87-1.89 5.37a7.73 7.73.0 01-2.39 1.9"/><path class="cls-2" d="M3.79 41a10.7 10.7.0 00-1.32 3.14c-.55 2.19-.23 4 .44 7.41.75 3.86 1.47 4.87 1.89 5.37a7.71 7.71.0 002.38 1.9"/><path class="cls-3" d="M50.58 6.9C41-1 29.61.49 27 .83A32.75 32.75.0 008.66 9.72c-2.31 2.26-6.13 6-7.56 12-2 8.23 1.37 17.47 3.26 21 0 .09.4.79.79 1.78.0.0.32.79.59 1.61.21.66.39 1.34.54 2a21 21 0 00.6-3.65c.24-3.61-.86-4-.74-8.74.06-2.47.24-9.92 4.15-11.85 1.55-.77 2.42.0 5.18.15 9.35.63 19.67-6.32 23.26-8.74a3.39 3.39.0 012.07-.89c3.55.18 3.42 9.28 10.37 14.37 1.78 1.3 2.32 1.1 3.41 2.37 3 3.48 1.28 7.75 2.08 11.41A7.72 7.72.0 0057.07 44c.28-.82.58-1.77.93-3l.13-.45a111.64 111.64.0 002.81-13.95C61.4 19.85 56.52 11.77 50.58 6.9z"/><path class="cls-2" d="M26.18 59.9a3.44 3.44.0 011.28-.18c1 .07 1.28.66 2.23 1.05a4.91 4.91.0 003.19.0c1.13-.39 1.24-1 2.26-1.21a3.5 3.5.0 012.06.31"/><path class="cls-1" d="M6.4 48.74c-.74-2.83 2.95-3.27 2.95-3.27"/><path class="cls-1" d="M55.82 48a2.57 2.57.0 00-2.07-3.1"/><path class="cls-2" d="M24.33 45.6c-1.82-2.56-8.19-1.65-9-.25"/><path class="cls-4" d="M18.63 45.58c0 1.83 3.05 1.83 3 0s-3.03-1.83-3 0z"/><path class="cls-2" d="M39.62 45.3c1.82-2.56 8.19-1.65 9-.24"/><path class="cls-4" d="M45.32 45.28c0 1.84-3 1.84-3 0s3.03-1.83 3 0z"/></g></g></svg></div><h3 title><a href=/>方舟</a></h3><div class=description><p>硕士研究生 @ 筑波大学<br>全栈工程师<br></p></div></div></div><ul class=social-links><li><a href=https://www.linkedin.com/in/zhou-fang-cn/ rel=me aria-label=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li><a href=https://github.com/fang2hou/ rel=me aria-label=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li><a href=https://twitter.com/fang2hou rel=me aria-label=twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li><a href=mailto:iszhoufang@gmail.com rel=me aria-label=e-mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul><div class=footer><div class=lang-switcher><p id=lang-notification></p><ul><li><a href=https://fang2hou.com/ onmouseover='languageNotify("中文")' onmouseleave=languageNotify(null) onclick='setLanguage("zh-cn")'><img class=flag src=/images/flags/zh-cn.svg></a></li><li><a href=https://fang2hou.com/ja/ onmouseover='languageNotify("日本語")' onmouseleave=languageNotify(null) onclick='setLanguage("ja")'><img class=flag src=/images/flags/ja.svg></a></li><li><a href=https://fang2hou.com/en/ onmouseover='languageNotify("English")' onmouseleave=languageNotify(null) onclick='setLanguage("en")'><img class=flag src=/images/flags/en.svg></a></li></ul></div><div class=by_farbox>&copy; fang2hou 2012-2021</div></div></div><div class=main><div class="page-top animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true></span><span aria-hidden=true></span><span aria-hidden=true></span></a><ul class=nav id=navMenu><li><a href=/ title>关于</a></li><li><a href=/post/ title>文章</a></li><li><a href=/project/ title>项目</a></li><li class=lang-switcher-nav><a href=/ title=中文><img class=flag src=/images/flags/zh-cn.svg> 中文</a></li><li class=lang-switcher-nav><a href=/ja/ title=日本語><img class=flag src=/images/flags/ja.svg> 日本語</a></li><li class=lang-switcher-nav><a href=/en/ title=English><img class=flag src=/images/flags/en.svg> English</a></li><li class=theme-switch-item><a class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></li></ul></div><div class=autopagerize_page_element><div class=content><div class="post animated fadeInDown"><div class=post-content><div class=post-title><h3>使用 Google Colab 提供的免费 TPU 进行训练</h3><div class=info><em class="fas fa-calendar-day"></em><span class=date>2019 年 10 月 9 日</span>
<em class="fas fa-stopwatch"></em><span class=reading-time>阅读时间 3 分钟</span></div></div><h1 id=前言>前言</h1><p>比起深度学习最初的 CPU 计算，现在普遍使用 GPU 来进行训练，效率提升已然是相当明显。但是像是 Google 和阿里巴巴这些科技巨头还是不满足这些，直接让日常使用介入芯片研发，Google 提出 TPU 方案，阿里也成立了平头哥独立品牌来发布新品。官方说法是对于目前顶尖 GPU 都有 20 倍以上的算力提升。<br>阿里和 Google 的芯片技术都已经实装运用在了自家的云服务上，不过有幸的是，Google 在自家推出的 Colab 机器学习平台之中提供了免费试用 TPU 的机会。这篇文章将会介绍一下当下，快速使用 Keras 进行训练的一个实例。<br>在看下面的文字的之前，最好回忆一下简单的 CNN 构造。如果你有一些 Keras 或是 TensorFlow 相关的使用经验，那么应该能够非常快速的理解。</p><h1 id=配置-colab-环境>配置 Colab 环境</h1><p>打开 <a href=https://colab.research.google.com>https://colab.research.google.com</a>，登陆谷歌账户就可以直接启动一个全新的运行实例。<br>由于是要使用 TPU，所以还要在菜单栏中的 <code>Runtime</code> -> <code>Change runtime type</code> 手动设定环境为 Python 3，硬件加速为 TPU。</p><p><img src=change_runtime.png alt=手动修改环境></p><p>Colab 不但可以承担 Jupyter Notebook 的工作，其实 Google 还为你准备了一个虚拟机。</p><p>在撰写本文时，TensorFlow 的正式版本为 1.14，但是我们需要执行的 Keras 支持训练命令(<code>model.fit_generator</code>)目前只适配到 1.13，所以先进行降级。<br>在第一个 Code cell 里填入下面的代码来实现降级，由于要先卸载掉 1.14，得稍微等待一下。</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># Downgrade tf to 1.13.1</span>
!pip install <span class=nv>tensorflow</span><span class=o>==</span>1.13.1
</code></pre></div><p>在降级成功后，Colab 会在输出之中会提示让你进行一次运行实例重启，点击 <code>RESTART RUNTIME</code> 即可。<br>有时候没有提示的话，就通过 <code>Runtime</code> -> <code>Restart runtime...</code> 重启。</p><h1 id=下载数据集>下载数据集</h1><p>在第二个 Code cell 里填入下面的代码来下载公开的测试数据集（猫狗大战，猫狗各 4000 训练集 + 1000 测试集）。</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># Download and unzip dataset</span>
!wget https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/P16-Convolutional-Neural-Networks.zip
!unzip -qq P16-Convolutional-Neural-Networks.zip <span class=s2>&#34;Convolutional_Neural_Networks/dataset/*&#34;</span>
!mv Convolutional_Neural_Networks/dataset dataset
!rm -rf Convolutional_Neural_Networks
</code></pre></div><h1 id=导入库>导入库</h1><p>这里导入 <code>os</code> 库用于 TPU 支持相关语句。<br><strong>注意：虽然我们是写 Keras 代码，但这里不能直接使用 <code>keras</code> 库中的模型，而是要使用 <code>tensorflow.keras</code>。</strong></p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>os</span>
<span class=kn>import</span> <span class=nn>tensorflow</span> <span class=kn>as</span> <span class=nn>tf</span>
<span class=kn>from</span> <span class=nn>tensorflow.contrib.tpu.python.tpu</span> <span class=kn>import</span> <span class=n>keras_support</span>
<span class=kn>from</span> <span class=nn>tensorflow.keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span>
<span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Conv2D</span><span class=p>,</span> <span class=n>MaxPooling2D</span><span class=p>,</span> <span class=n>Flatten</span><span class=p>,</span> <span class=n>Dense</span><span class=p>,</span> <span class=n>Dropout</span>
<span class=kn>from</span> <span class=nn>keras.preprocessing.image</span> <span class=kn>import</span> <span class=n>ImageDataGenerator</span>
</code></pre></div><h1 id=数据集预处理>数据集预处理</h1><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Image processing</span>
<span class=n>train_datagen</span> <span class=o>=</span> <span class=n>ImageDataGenerator</span><span class=p>(</span><span class=n>rescale</span><span class=o>=</span><span class=mf>1.</span> <span class=o>/</span><span class=mi>255</span><span class=p>,</span>
                                   <span class=n>shear_range</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>
                                   <span class=n>zoom_range</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span>
                                   <span class=n>horizontal_flip</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>

<span class=n>test_datagen</span> <span class=o>=</span> <span class=n>ImageDataGenerator</span><span class=p>(</span><span class=n>rescale</span><span class=o>=</span><span class=mf>1.</span> <span class=o>/</span> <span class=mi>255</span><span class=p>)</span>

<span class=n>train_data</span> <span class=o>=</span> <span class=n>train_datagen</span><span class=o>.</span><span class=n>flow_from_directory</span><span class=p>(</span><span class=s1>&#39;dataset/training_set&#39;</span><span class=p>,</span>
                                               <span class=n>target_size</span><span class=o>=</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
                                               <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
                                               <span class=n>class_mode</span><span class=o>=</span><span class=s1>&#39;binary&#39;</span><span class=p>)</span>

<span class=n>validation_data</span> <span class=o>=</span> <span class=n>test_datagen</span><span class=o>.</span><span class=n>flow_from_directory</span><span class=p>(</span><span class=s1>&#39;dataset/test_set&#39;</span><span class=p>,</span>
                                                   <span class=n>target_size</span><span class=o>=</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>),</span>
                                                   <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
                                                   <span class=n>class_mode</span><span class=o>=</span><span class=s1>&#39;binary&#39;</span><span class=p>)</span>
</code></pre></div><p>很容易理解吧，这里的代码主要来自于 Keras 的官方文档。<br>我打算采用 (128, 128, 3) 的格式输入学习数据，每批个数为 32。你可以根据自己的想法进行调整。<br>如果一切顺利，你可以看到下面的输出。</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>Found 8000 images belonging to 2 classes.
Found 2000 images belonging to 2 classes.
</code></pre></div><h1 id=构建网络>构建网络</h1><p>一个简单的卷积网络<br>3 个卷积层，每次都进行池化，全连接层习惯性用2个。</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>build_nn</span><span class=p>():</span>
    <span class=c1># Build the CNN</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>kernel_initializer</span><span class=o>=</span><span class=s1>&#39;he_uniform&#39;</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>3</span><span class=p>)))</span>
    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>MaxPooling2D</span><span class=p>(</span><span class=n>pool_size</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>kernel_initializer</span><span class=o>=</span><span class=s1>&#39;he_uniform&#39;</span><span class=p>))</span>
    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>MaxPooling2D</span><span class=p>(</span><span class=n>pool_size</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv2D</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>),</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>,</span> <span class=n>kernel_initializer</span><span class=o>=</span><span class=s1>&#39;he_uniform&#39;</span><span class=p>))</span>
    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>MaxPooling2D</span><span class=p>(</span><span class=n>pool_size</span><span class=o>=</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Flatten</span><span class=p>())</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.2</span><span class=p>))</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.2</span><span class=p>))</span>

    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>))</span>
    
    <span class=k>return</span> <span class=n>model</span>
</code></pre></div><h1 id=初始化-tpu-环境>初始化 TPU 环境</h1><p>先取得 TPU 的位置，这套用法基本上是固定的，每次要用的时候 copy-paste 就行。</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Initialize TPU</span>
<span class=n>tpu_grpc_url</span> <span class=o>=</span> <span class=s2>&#34;grpc://&#34;</span><span class=o>+</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;COLAB_TPU_ADDR&#34;</span><span class=p>]</span>
<span class=n>tpu_cluster_resolver</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>cluster_resolver</span><span class=o>.</span><span class=n>TPUClusterResolver</span><span class=p>(</span><span class=n>tpu_grpc_url</span><span class=p>)</span>
<span class=n>strategy</span> <span class=o>=</span> <span class=n>keras_support</span><span class=o>.</span><span class=n>TPUDistributionStrategy</span><span class=p>(</span><span class=n>tpu_cluster_resolver</span><span class=p>)</span>
</code></pre></div><p>成功运行的话，可以看到，显示返回到了地址，然后从地址去获取设备信息，1 Worker 8 Core 的 TPU。注意这个数字，因为这意味之后测试集需要组成 batch 为 8 的倍数才能传入。</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>INFO:tensorflow:Querying Tensorflow master (grpc://10.110.24.138:8470) for TPU system metadata.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14991867850935745303)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4977701856257024240)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 799391477980569210)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7413645770321013063)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12127959108352401067)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9301267841357501458)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2329118821990200537)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6102181154716035854)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6202014608559033004)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7250002334323814010)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16188795321964965491)
</code></pre></div><h1 id=训练前的准备>训练前的准备</h1><p>终于到了要训练的时候，不过注意，TPU 的模型不同于 GPU 和 CPU 的模型关系，TPU 模型需要特殊的转换才能跑，而且模型也会损失掉部分方法，之前我们降级实际上就是为了使用在 1.14 中目前不支持的函数。相信之后 TPU 的时候会越来越方便，毕竟现在还处于发展初期。</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Convert cpu model to tpu model</span>
<span class=n>myOptimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>optimizers</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
<span class=n>my_cnn</span> <span class=o>=</span> <span class=n>build_nn</span><span class=p>()</span>
<span class=n>my_cnn</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>myOptimizer</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
<span class=n>my_cnn</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>tpu</span><span class=o>.</span><span class=n>keras_to_tpu_model</span><span class=p>(</span><span class=n>my_cnn</span><span class=p>,</span> <span class=n>strategy</span><span class=o>=</span><span class=n>strategy</span><span class=p>)</span>
</code></pre></div><p>这段代码中有个非常容易被忽视的点，那就是optimizer必须使用到 TensorFlow 的，而不是 Keras 的，否则就会报错哦！我这里使用的是 <code>Adam</code> 算法，这里特别说下，常用的随机梯度下降（SGD）目前在 TensorFlow 中映射到 keras 支持模块，不能直接拿来用。</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Convert CPU model to TPU model</span>
<span class=n>myOptimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>AdamOptimizer</span><span class=p>()</span>
<span class=n>my_cnn</span> <span class=o>=</span> <span class=n>build_nn</span><span class=p>()</span>
<span class=n>my_cnn</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=n>myOptimizer</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
<span class=n>my_cnn</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>tpu</span><span class=o>.</span><span class=n>keras_to_tpu_model</span><span class=p>(</span><span class=n>my_cnn</span><span class=p>,</span> <span class=n>strategy</span><span class=o>=</span><span class=n>strategy</span><span class=p>)</span>
</code></pre></div><p>执行成功的话就会看到下面的文字：</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.
</code></pre></div><h1 id=训练咖啡时间>训练，咖啡时间</h1><p><code>epoch</code> 要多少自己看着设定，第一轮训练其实是较慢的，第二次开始时，速度比起传统 GPU 训练可以说是非常之快速了。</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Train!</span>
<span class=n>my_cnn</span><span class=o>.</span><span class=n>fit_generator</span><span class=p>(</span><span class=n>train_data</span><span class=p>,</span>
                     <span class=n>steps_per_epoch</span><span class=o>=</span><span class=mi>8000</span><span class=p>,</span>
                     <span class=n>epochs</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
                     <span class=n>validation_data</span><span class=o>=</span><span class=n>validation_data</span><span class=p>,</span>
                     <span class=n>validation_steps</span><span class=o>=</span><span class=mi>2000</span><span class=p>)</span>
</code></pre></div><h1 id=保存权重>保存权重</h1><p>尽量不要直接使用保存全网络的函数，保存权重即可。<br>测试可以在本地构建一个 CPU 模型导入权重来进行测试。</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>classifier</span><span class=o>.</span><span class=n>save_weights</span><span class=p>(</span><span class=s1>&#39;weights.h5&#39;</span><span class=p>,</span> <span class=n>overwrite</span><span class=o>=</span><span class=bp>True</span><span class=p>)</span>
</code></pre></div><h1 id=结论>结论</h1><p>专用硬件训练网络体验真的很棒！只是现在外部软件支持还没有做的很好，但是足以让人期待之后的表现的。<br>深度学习从几年前的实装困难，到现在简单几句就能开始训练，确确实实从学术界普及到了日常生活之中。很多初学者手上没有足够的硬件导致学习效率低下，相信随着芯片技术的发展，这个问题将不复存在。</p></div><div class=post-footer><div class=info><span class=separator><a class=tag href=/tags/cnn/>CNN</a><a class=tag href=/tags/deep-learning/>Deep Learning</a><a class=tag href=/tags/tpu/>TPU</a></span></div></div></div></div></div></div><script type=text/javascript src=/js/medium-zoom.min.2d6fd0be87fa98f0c9b4dc2536b312bbca48757f584f6ea1f394abc9bcc38fbc.js integrity="sha256-LW/Qvof6mPDJtNwlNrMSu8pIdX9YT26h85SrybzDj7w=" crossorigin=anonymous></script></body></html>